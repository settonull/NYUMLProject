{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "conf_dir = 'conferences'\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "\n",
    "def load_csvs(file_names):\n",
    "    \"\"\"Loads and concatentates csv's from a directory\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for each_file in file_names:\n",
    "        new_df = pd.read_csv(each_file)\n",
    "        df = pd.concat([df, new_df])\n",
    "    return df\n",
    "\n",
    "def join_data(scores_df, stats_df, odds_df):\n",
    "    \"\"\"\n",
    "    Creates a unique key for each game using the date the game was played\n",
    "    and the home and away abbreviated names (Not all data sets have a HomeID\n",
    "    and AwayID)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add dates to join on\n",
    "    scores_df['Year'] = scores_df['WeekDate'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").year)\n",
    "    scores_df['Month'] = scores_df['WeekDate'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").month)\n",
    "    scores_df['Day'] = scores_df['WeekDate'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").day)\n",
    "    stats_df['Year'] = stats_df['Start'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").year)\n",
    "    stats_df['Month'] = stats_df['Start'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").month)\n",
    "    stats_df['Day'] = stats_df['Start'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").day)\n",
    "    odds_df['Year'] = odds_df['DATE(date)'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").year)\n",
    "    odds_df['Month'] = odds_df['DATE(date)'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").month)\n",
    "    odds_df['Day'] = odds_df['DATE(date)'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").day)\n",
    "\n",
    "    # Join Data\n",
    "    data = scores_df.merge(\n",
    "        stats_df.drop(['Season', 'Start', 'Week'], axis=1),\n",
    "        left_on = ['Year', 'Month', 'Day', 'Home', 'Visiter'],\n",
    "        right_on = ['Year', 'Month', 'Day', 'Home', 'Away'])\n",
    "    data = data.merge(odds_df.drop(['DATE(date)', 'HomeScore', 'AwayScore'],\n",
    "                                    axis=1),\n",
    "        left_on = ['Year', 'Month', 'Day', 'Home', 'Visiter'],\n",
    "        right_on = ['Year', 'Month', 'Day', 'Home', 'Away'])\n",
    "\n",
    "    # Target feature\n",
    "    data['target_margin'] = data['HomeFinal'] - data['VisFinal']\n",
    "\n",
    "    # Other features\n",
    "    data['D1_Match'] = [True if not pd.isnull(x) else False for \\\n",
    "                        x in data['Spread_Mirage']]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load data locations\n",
    "scores_dir = 'scores_pe'\n",
    "stats_dir = 'stats'\n",
    "odds_dir = ''\n",
    "\n",
    "scores_names = glob.glob(os.path.join(root_dir, data_dir, scores_dir, \"scores_pythElo201?.csv\"))\n",
    "stats_names =  glob.glob(os.path.join(root_dir, data_dir, stats_dir, \"ncaastats201?.csv\"))\n",
    "odds_names = [os.path.join(root_dir, data_dir, odds_dir, \"NCAAF_Odds.csv\")]\n",
    "\n",
    "# Import data and join\n",
    "scores_df = load_csvs(scores_names)\n",
    "stats_df = load_csvs(stats_names)\n",
    "odds_df = load_csvs(odds_names)\n",
    "data = join_data(scores_df, stats_df, odds_df)\n",
    "\n",
    "spreads = data.set_index(['HomeID','VisID','Season','Week']).filter(regex=\"Spread_\")\n",
    "#m = spreads.mean(axis=1)\n",
    "#for i, col in enumerate(spreads):\n",
    "    # using i allows for duplicate columns\n",
    "    # inplace *may* not always work here, so IMO the next line is preferred\n",
    "    # df.iloc[:, i].fillna(m, inplace=True)\n",
    "    #spreads.iloc[:, i] = spreads.iloc[:, i].fillna(m)\n",
    "# spreads['target_margin'] = data['target_margin']\n",
    "#spreads.dropna(axis=0, inplace=True)\n",
    "spreads = spreads.join(pd.DataFrame(data.set_index(['HomeID','VisID','Season','Week'])['target_margin']))\n",
    "\n",
    "# Join Conference Data\n",
    "file = os.path.join(data_dir, conf_dir, \"mergedConferences.csv\")\n",
    "conf_df = pd.read_csv(file).drop_duplicates()\n",
    "spreads= spreads.reset_index().merge(conf_df,\n",
    "                                            left_on=['HomeID', 'Season'],\n",
    "                                            right_on=['ID','Year'],\n",
    "                                            suffixes=('','Home'))\n",
    "spreads = spreads.reset_index().merge(conf_df,\n",
    "                                            left_on=['VisID', 'Season'],\n",
    "                                            right_on=['ID','Year'],\n",
    "                                            suffixes=('','Vis'))\n",
    "spreads['Week'] = spreads['Week'].astype(int)\n",
    "spreads['Week'] = np.where(spreads['Season']==2016, spreads['Week'] - 1, spreads['Week'])\n",
    "#spreads = spreads.set_index(['HomeID', 'VisID', 'Season', 'Week'])\n",
    "spreads = spreads.drop(['ID','Year','IDVis','index','Team','TeamVis','ConfVis','Year','YearVis'],1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "casinos = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "                'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "                'Spread_SBG', 'Spread_BetUS']\n",
    "\n",
    "spreads['SpreadMed'] = spreads[casinos].median(axis=1)\n",
    "spreads['SpreadMode'] = spreads[casinos].mode(axis=1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT FOR USE IN R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for y, g in spreads.groupby('Season'):\n",
    "    weeks = [group for _, group in g.groupby('Week')]\n",
    "    for i, w in enumerate(weeks):\n",
    "        i += 1\n",
    "        if i == 5:\n",
    "            pd.concat(weeks[:i]).to_csv('data/new_odds/pre/pre_{}/odds{}_{}.csv'.format(y, y, i), index=False)\n",
    "        elif i>5:\n",
    "            w.to_csv('data/new_odds/pre/pre_{}/odds{}_{}.csv'.format(y, y, i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECOMBINE AFTER R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "new_nnList = [0 for i in range(4)]\n",
    "\n",
    "for j, n in enumerate(range(2,6)):\n",
    "    new_oddsList = [0 for i in range(4)]\n",
    "    for i, yr in enumerate(range(2013,2017)):\n",
    "        lst_wk = spreads.loc[spreads['Season']==yr,'Week'].max()\n",
    "        new_oddsList[i] = pd.read_csv('data/new_odds/post/post_{}/odds{}_{}_{}.csv'.format(yr, yr, lst_wk, n))\n",
    "    \n",
    "    new_nnList[j] = pd.concat(new_oddsList)\n",
    "    new_nnList[j]['Spread_Med2'] = new_nnList[j][casinos].median(axis=1)\n",
    "    new_nnList[j]['Spread_Mode2'] = new_nnList[j][casinos].mode(axis=1)[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import *\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_casinos = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "                'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "                'Spread_SBG', 'Spread_BetUS']\n",
    "\n",
    "orig_med = ['Spread_Med']\n",
    "orig_mode = ['Spread_Mode']\n",
    "orig_summaries = ['Spread_Med', 'Spread_Mode']\n",
    "orig_total = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "               'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "               'Spread_SBG', 'Spread_BetUS', 'Spread_Med', 'Spread_Mode']\n",
    "\n",
    "new_med = ['Spread_Med2']\n",
    "new_mode = ['Spread_Mode2']\n",
    "new_rec = ['Spread_Rec']\n",
    "new_summaries = ['Spread_Med2', 'Spread_Mode2']\n",
    "new_total = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "             'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "             'Spread_SBG', 'Spread_BetUS', \n",
    "             'Spread_Med', 'Spread_Mode', 'Spread_Rec']\n",
    "\n",
    "new_total_total = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "               'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "               'Spread_SBG', 'Spread_BetUS', 'Spread_Med', 'Spread_Mode',\n",
    "                'Spread_Med2', 'Spread_Mode2', 'Spread_Rec']\n",
    "\n",
    "features = [('orig_casinos', orig_casinos), ('orig_summaries', orig_summaries), ('orig_total', orig_total),\n",
    "            ('orig_med', orig_med), ('orig_mode', orig_mode), \n",
    "            ('new_med', new_med), ('new_mode', new_mode), \n",
    "            ('new_summaries', new_summaries), ('new_total', new_total), ('new_rec', new_rec),\n",
    "            ('new_total_total',new_total_total)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's the best nn?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for nn: 2\n",
      "orig_casinos: 283.2016754426179\n",
      "orig_summaries: 272.3366434836036\n",
      "orig_total: 290.2088879050677\n",
      "orig_med: 272.2493773623063\n",
      "orig_mode: 272.2827716719263\n",
      "new_med: 317.0148668191828\n",
      "new_mode: 297.9693020273856\n",
      "new_summaries: 300.43638899094105\n",
      "new_total: 270.90237413115915\n",
      "new_rec: 425.5254904407057\n",
      "new_total_total: 272.3969770544182\n",
      "*****************************************************\n",
      "Results for nn: 3\n",
      "orig_casinos: 288.8386342518825\n",
      "orig_summaries: 272.3366434836036\n",
      "orig_total: 294.3823559019624\n",
      "orig_med: 272.2493773623063\n",
      "orig_mode: 272.2827716719263\n",
      "new_med: 317.119491003821\n",
      "new_mode: 299.74308140732523\n",
      "new_summaries: 301.26496411682405\n",
      "new_total: 283.5560140094089\n",
      "new_rec: 430.96058421973464\n",
      "new_total_total: 285.96907191419\n",
      "*****************************************************\n",
      "Results for nn: 4\n",
      "orig_casinos: 289.7095097342245\n",
      "orig_summaries: 272.3366434836036\n",
      "orig_total: 294.05749100560746\n",
      "orig_med: 272.2784528139681\n",
      "orig_mode: 272.2827716719263\n",
      "new_med: 317.2312054971936\n",
      "new_mode: 295.8222407915741\n",
      "new_summaries: 298.73776052918765\n",
      "new_total: 287.2525976500307\n",
      "new_rec: 449.5791388368595\n",
      "new_total_total: 293.7056625727075\n",
      "*****************************************************\n",
      "Results for nn: 5\n",
      "orig_casinos: 283.04659508537475\n",
      "orig_summaries: 272.3366434836036\n",
      "orig_total: 283.8381669263015\n",
      "orig_med: 272.2881308883775\n",
      "orig_mode: 272.2827716719263\n",
      "new_med: 314.59529838740553\n",
      "new_mode: 294.3581079661778\n",
      "new_summaries: 297.0394652284626\n",
      "new_total: 302.39862447188773\n",
      "new_rec: 449.2301064845176\n",
      "new_total_total: 304.994957767323\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "for i, nn in enumerate(range(2,6)):\n",
    "    print('Results for nn: {}'.format(nn))\n",
    "    for name, feats in features:\n",
    "        new_feats = feats.copy()\n",
    "        new_feats.append('Season')\n",
    "        new_feats.append('target_margin')\n",
    "        new_spreads = new_nnList[i][new_feats].copy().dropna()\n",
    "\n",
    "        X_train = new_spreads.loc[new_spreads['Season']<2016,feats]\n",
    "        X_val = new_spreads.loc[new_spreads['Season']==2016,feats]\n",
    "        y_train = new_spreads.loc[new_spreads['Season']<2016,'target_margin']\n",
    "        y_val = new_spreads.loc[new_spreads['Season']==2016,'target_margin']\n",
    "\n",
    "        standardscaler = StandardScaler()\n",
    "        X_trainS = standardscaler.fit_transform(X_train)\n",
    "        X_valS = standardscaler.transform(X_val)\n",
    "        \n",
    "    \n",
    "        print('{}: {}' .format(name, \n",
    "                               mean_squared_error(y_val, LinearRegression().fit(X_trainS, y_train).predict(X_valS))))\n",
    "    print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_spreads = new_nnList[0]\n",
    "new_spreads.to_csv('data/new_odds/new_odds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Out-of-the-box Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_casinos: 293.6657070410407\n",
      "*****************************************************\n",
      "orig_summaries: 323.91335619412985\n",
      "*****************************************************\n",
      "orig_total: 293.9583650617588\n",
      "*****************************************************\n",
      "orig_med: 323.8073677617688\n",
      "*****************************************************\n",
      "orig_mode: 323.6316464408711\n",
      "*****************************************************\n",
      "new_med: 355.20891382311106\n",
      "*****************************************************\n",
      "new_mode: 344.66858318887574\n",
      "*****************************************************\n",
      "new_summaries: 344.41061926852336\n",
      "*****************************************************\n",
      "new_total: 275.39117511175635\n",
      "*****************************************************\n",
      "new_rec: 440.86547681091815\n",
      "*****************************************************\n",
      "new_total_total: 277.2248937946552\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "for name, feats in features:\n",
    "    new_feats = feats.copy()\n",
    "    new_feats.append('Season')\n",
    "    new_feats.append('target_margin')\n",
    "    new_spreadsTemp = new_spreads[new_feats].copy().dropna()\n",
    "\n",
    "    X_train = new_spreadsTemp.loc[new_spreadsTemp['Season']<2016,feats]\n",
    "    X_val = new_spreadsTemp.loc[new_spreadsTemp['Season']==2016,feats]\n",
    "    y_train = new_spreadsTemp.loc[new_spreadsTemp['Season']<2016,'target_margin']\n",
    "    y_val = new_spreadsTemp.loc[new_spreadsTemp['Season']==2016,'target_margin']\n",
    "\n",
    "    standardscaler = StandardScaler()\n",
    "    X_trainS = standardscaler.fit_transform(X_train)\n",
    "    X_valS = standardscaler.transform(X_val)\n",
    "        \n",
    "    \n",
    "    print('{}: {}' .format(name, \n",
    "                           mean_squared_error(y_val, KernelRidge().fit(X_trainS, y_train).predict(X_valS))))\n",
    "    print('*****************************************************')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def oddsGridSearch(new_spreads, estimators):\n",
    "    for name, feats, est, params in estimators:\n",
    "        new_feats = feats.copy()\n",
    "        new_feats.append('Season')\n",
    "        new_feats.append('target_margin')\n",
    "        new_spreadTemp = new_spreads[new_feats].copy().dropna()\n",
    "\n",
    "        X_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,feats]\n",
    "        X_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,feats]\n",
    "        y_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,'target_margin']\n",
    "        y_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,'target_margin']\n",
    "\n",
    "        standardscaler = StandardScaler()\n",
    "        X_trainS = standardscaler.fit_transform(X_train)\n",
    "        X_valS = standardscaler.transform(X_val)\n",
    "\n",
    "        X_train_val = np.vstack((X_trainS, X_valS))\n",
    "        y_train_val = np.concatenate((y_train, y_val))\n",
    "        val_fold = [-1]*len(X_trainS) + [0]*len(X_valS) #0 corresponds to validation\n",
    "        grid = GridSearchCV(est,\n",
    "                            params,\n",
    "                            return_train_score=False,\n",
    "                            cv = PredefinedSplit(test_fold=val_fold),\n",
    "                            refit = True,\n",
    "                            scoring = make_scorer(mean_squared_error, greater_is_better = False))\n",
    "        grid.fit(X_train_val, y_train_val)\n",
    "        bestimator = grid.best_estimator_\n",
    "        print(name)\n",
    "        print(grid.best_params_ )\n",
    "        print('MSE: {}' .format(mean_squared_error(y_val, bestimator.fit(X_trainS, y_train).predict(X_valS))))\n",
    "        print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_summaries\n",
      "{'alpha_1': 1.0, 'alpha_2': 1e-10, 'lambda_1': 5.6234132519034905e-08, 'lambda_2': 1.0}\n",
      "MSE: 272.2890741636246\n",
      "*****************************************************\n",
      "new_total\n",
      "{'alpha_1': 100.0, 'alpha_2': 5.623413251903491e-09, 'lambda_1': 3.1622776601683795e-09, 'lambda_2': 316.22776601683796}\n",
      "MSE: 268.38235593292285\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "oddsGridSearch(new_spreads, \n",
    "    [('orig_summaries',orig_summaries, BayesianRidge(), [{'alpha_1':10**np.arange(0, 3, 0.25),\n",
    "                                                        'alpha_2':10**np.arange(-10, -7, 0.25),\n",
    "                                                        'lambda_1':10**np.arange(-10, -7, 0.25),\n",
    "                                                        'lambda_2':10**np.arange(0, 3, 0.25)}]),\n",
    "     ('new_total',new_total, BayesianRidge(), [{'alpha_1':10**np.arange(0, 3, 0.25),\n",
    "                                              'alpha_2':10**np.arange(-10, -7, 0.25),\n",
    "                                              'lambda_1':10**np.arange(-10, -7, 0.25),\n",
    "                                              'lambda_2':10**np.arange(0, 3, 0.25)}])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268.38235593292285"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feats = new_total.copy()\n",
    "new_feats.append('Season')\n",
    "new_feats.append('target_margin')\n",
    "new_spreadTemp = new_spreads[new_feats].copy().dropna()\n",
    "\n",
    "X_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,feats]\n",
    "X_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,feats]\n",
    "y_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,'target_margin']\n",
    "y_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,'target_margin']\n",
    "\n",
    "standardscaler = StandardScaler()\n",
    "X_trainS = standardscaler.fit_transform(X_train)\n",
    "X_valS = standardscaler.transform(X_val)\n",
    "\n",
    "br = BayesianRidge(alpha_1=100.0, alpha_2=5.623413251903491e-09, \n",
    "              lambda_1=3.1622776601683795e-09, lambda_2=316.22776601683796).fit(X_trainS, y_train)\n",
    "mean_squared_error(y_val, br.predict(X_valS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_summaries\n",
      "{'alpha': 17.78279410038923, 'epsilon': 9.5}\n",
      "MSE: 271.1672110862091\n",
      "*****************************************************\n",
      "new_total\n",
      "{'alpha': 0.1778279410038923, 'epsilon': 1.75}\n",
      "MSE: 267.57427540354036\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "oddsGridSearch(new_spreads, \n",
    "    [('orig_summaries',orig_summaries, HuberRegressor(), [{'epsilon':np.arange(1.0, 10, 0.25),\n",
    "                                                           'alpha':10**np.arange(-7,3,0.25)}]),\n",
    "     ('new_total',new_total, HuberRegressor(), [{'epsilon':np.arange(1.0, 10, 0.25),\n",
    "                                                'alpha':10**np.arange(-7,3,0.25)}])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_summaries\n",
      "{'alpha': 3.1622776601683795, 'gamma': 0.01}\n",
      "MSE: 271.0657360782656\n",
      "*****************************************************\n",
      "new_total\n",
      "{'alpha': 3.1622776601683795}\n",
      "MSE: 275.2552739053091\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "oddsGridSearch(new_spreads, \n",
    "    [('orig_summaries',orig_summaries, KernelRidge(kernel='rbf'), [{'alpha':10**np.arange(-7,3,0.5),\n",
    "                                                                    'gamma':10**np.arange(-7,3,0.5)}]),\n",
    "     ('new_total',new_total, KernelRidge(kernel='linear'), [{'alpha':10**np.arange(-7,3,0.5)}])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
