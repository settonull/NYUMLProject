{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from construct_data import load_csvs\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "ctmc_dir = 'ctmc'\n",
    "glicko_dir = 'glicko'\n",
    "curseason_dir = 'curseason'\n",
    "scorepreds_dir = 'scorepreds'\n",
    "snooz_dir = 'snoozle'\n",
    "bcs_dir = 'BCS'\n",
    "conf_dir = 'conferences'\n",
    "odds_dir = 'new_odds'\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "##### IJH Data\n",
    "file = os.path.join(data_dir, snooz_dir, \"snoozle_ijh.csv\")\n",
    "snooz_df = pd.read_csv(file)\n",
    "snooz_df = snooz_df.set_index(['HomeID', 'VisID', 'Season', 'Week'])\n",
    "snooz_df['target_margin'] = snooz_df['HomeFinal'] - snooz_df['VisFinal']\n",
    "snooz_df = snooz_df[['HomeElo', 'HomeEloProb','HomeLuck','HomePrevLuck',\n",
    "                            'HomePythPct','HomePythWins','HomeWinPct','VisElo',\n",
    "                            'VisEloProb','VisLuck', 'VisPrevLuck','VisPythPct',\n",
    "                            'VisPythWins','VisWinPct','SpreadElo',\n",
    "                            'target_margin','HomeFinal','VisFinal']]\n",
    "snooz_df = snooz_df.drop_duplicates()\n",
    "\n",
    "####Odds\n",
    "file = os.path.join(data_dir, odds_dir, 'new_odds.csv')\n",
    "odds_df = pd.read_csv(file, index_col=[0,1,2,3]).drop_duplicates()\n",
    "odds_df.drop(['target_margin','Conf'], axis=1, inplace=True)\n",
    "\n",
    "######## SWC Data\n",
    "file = os.path.join(data_dir, ctmc_dir, \"score_ctmc_snoozle.csv\")\n",
    "scores_ctmc_df = pd.read_csv(file,index_col=[0,1,2,3]).drop_duplicates()\n",
    "scores_ctmc_df.index.names = ['HomeID', 'VisID', 'Season', 'Week']\n",
    "file = os.path.join(data_dir, glicko_dir, \"glicko_snoozle.csv\")\n",
    "glicko_df = pd.read_csv(file, index_col=[0,1,2,3]).drop_duplicates()\n",
    "glicko_df.index.names = ['HomeID', 'VisID', 'Season', 'Week']\n",
    "file = os.path.join(data_dir, curseason_dir, \"curseason.csv\")\n",
    "curseason_df = pd.read_csv(file, index_col=[0,1,2,3]).drop_duplicates()\n",
    "file = os.path.join(data_dir, scorepreds_dir, \"scorepreds.csv\")\n",
    "scorepreds_df = pd.read_csv(file, index_col=[0,1,2,3]).drop_duplicates()\n",
    "\n",
    "# Join SC Data\n",
    "data_final = odds_df.join(snooz_df, how='left')\n",
    "data_final = data_final.join(scores_ctmc_df, how='left')\n",
    "data_final = data_final.join(glicko_df, how='left')\n",
    "data_final = data_final.join(curseason_df, how='left')\n",
    "data_final = data_final.join(scorepreds_df, how='left')\n",
    "\n",
    "##### CDR Data\n",
    "file = os.path.join(data_dir, bcs_dir, \"BCS-SOS.csv\")\n",
    "bcs_df = pd.read_csv(file).drop_duplicates()\n",
    "bcs_df = bcs_df.set_index(['HomeID', 'VisID', 'Season', 'Week'])\n",
    "\n",
    "# Join CDR Data\n",
    "data_final = data_final.join(bcs_df)\n",
    "\n",
    "# Join Conference Data\n",
    "file = os.path.join(data_dir, conf_dir, \"mergedConferences.csv\")\n",
    "conf_df = pd.read_csv(file).drop_duplicates()\n",
    "data_final = data_final.reset_index().merge(conf_df,\n",
    "                                            left_on=['HomeID', 'Season'],\n",
    "                                            right_on=['ID','Year'],\n",
    "                                            suffixes=('','Home'))\n",
    "data_final = data_final.reset_index().merge(conf_df,\n",
    "                                            left_on=['VisID', 'Season'],\n",
    "                                            right_on=['ID','Year'],\n",
    "                                            suffixes=('','Vis'))\n",
    "data_final = data_final.set_index(['HomeID', 'VisID', 'Season', 'Week'])\n",
    "data_final = data_final.drop(['ID','Year','IDVis','index'],1)\n",
    "\n",
    "\n",
    "# Impute HomeConf Data\n",
    "data_final['HomeConf_NotMajor'] = np.where(data_final['Conf'] == 'NotMajor', 1, 0)\n",
    "data_final['VisConf_NotMajor'] = np.where(data_final['ConfVis'] == 'NotMajor', 1, 0)\n",
    "data_final['PredSpread'] = data_final['HomePredFinal'] - data_final['VisPredFinal']\n",
    "\n",
    "glicko_diff = data_final['Glicko_Rating_Home'] - data_final['Glicko_Rating_Away']\n",
    "data_final['SpreadGlicko'] = (glicko_diff)/25 + 2.6 #predicted spread\n",
    "data_final['HomeGlickoProb'] = 1/(10**(-glicko_diff/400)+1)\n",
    "data_final['VisGlickoProb'] = 1/(10**(glicko_diff/400)+1)\n",
    "\n",
    "################################################################################\n",
    "# Train - Val - Test Splits\n",
    "################################################################################\n",
    "\n",
    "data_final_clean = data_final.copy().dropna()\n",
    "data_final_clean.reset_index(inplace=True)\n",
    "data_final_clean.sort_values(['Season','Week'])\n",
    "#data_final_clean.set_index(['HomeID', 'VisID', 'Season', 'Week'], inplace=True)\n",
    "\n",
    "X_train = data_final_clean.loc[(data_final_clean['Season']<2016) & \\\n",
    "                     (data_final_clean['Week'] > 4)].\\\n",
    "                                       drop(['target_margin'], axis=1)\n",
    "    \n",
    "y_train = data_final_clean.loc[(data_final_clean['Season']<2016) & \\\n",
    "                     (data_final_clean['Week'] > 4), 'target_margin']\n",
    "\n",
    "X_val = data_final_clean.loc[(data_final_clean['Season']==2016) & \\\n",
    "                     (data_final_clean['Week'] > 4)].\\\n",
    "                                       drop(['target_margin'], axis=1)\n",
    "    \n",
    "y_val = data_final_clean.loc[(data_final_clean['Season']==2016) & \\\n",
    "                     (data_final_clean['Week'] > 4), 'target_margin']\n",
    "\n",
    "#X_test = data_final[(data_final.index.get_level_values(2)==2017) & \\\n",
    "#                    (data_final['Conf']!='NotMajor') & \\\n",
    "#                    (data_final.index.get_level_values(3)>4)].\\\n",
    "#                                      drop(['target_margin'], axis=1).\\\n",
    "#                                      fillna(data_final.mean())\n",
    "#y_test = data_final[(data_final.index.get_level_values(2)==2017) & \\\n",
    "#                    (data_final['Conf']!='NotMajor') & \\\n",
    "#                    (data_final.index.get_level_values(3)>4)]['target_margin']\n",
    "\n",
    "# Current season average margins diff\n",
    "X_train['CurSeason_HH_VA_mean_margin'] = (X_train['SpreadHomeInSeasonAvg'] \\\n",
    "                                    - X_train['SpreadVisInSeasonAvg']*-1)\n",
    "X_val['CurSeason_HH_VA_mean_margin'] = (X_val['SpreadHomeInSeasonAvg'] \\\n",
    "                                        - X_val['SpreadVisInSeasonAvg']*-1)\n",
    "#X_test['CurSeason_HH_VA_mean_margin'] = (X_test['SpreadHomeInSeasonAvg'] \\\n",
    "#                                        - X_test['SpreadVisInSeasonAvg']*-1)\n",
    "\n",
    "################################################################################\n",
    "# Final Feature Selection\n",
    "################################################################################\n",
    "\n",
    "base_features = [col for col in data_final.columns if \\\n",
    "                      col.find('InSeason') > -1]\n",
    "\n",
    "swc_features = ['Glicko_Rating_Home', 'Glicko_Rating_Away',\n",
    "                'Glicko_Rating_Deviance_Home', 'Glicko_Rating_Deviance_Away',\n",
    "                'Glicko_Sigma_Home', 'Glicko_Sigma_Away',\n",
    "                'CTMC_Rating_Home', 'CTMC_Rating_Away','HomePredFinal',\n",
    "                'VisPredFinal','CurSeason_HH_VA_mean_margin',\n",
    "                'HomeOddsHomeInSeasonAvg','HomeOddsVisInSeasonAvg']\n",
    "\n",
    "ijh_features = ['HomeElo', 'HomeEloProb','HomeLuck','HomePrevLuck',\n",
    "                'HomePythPct','HomePythWins','HomeWinPct','VisElo',\n",
    "                'VisEloProb','VisLuck', 'VisPrevLuck','VisPythPct',\n",
    "                'VisPythWins','VisWinPct','SpreadElo',\n",
    "                'HomeConf_NotMajor','VisConf_NotMajor']\n",
    "\n",
    "odds_features= ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "                'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "                'Spread_SBG', 'Spread_BetUS', 'Spread_Med', 'Spread_Mode']\n",
    "\n",
    "test_features = ['SpreadGlicko', 'HomeGlickoProb', 'VisGlickoProb']\n",
    "\n",
    "cdr_features = bcs_df.columns.values.tolist()\n",
    "\n",
    "# featurestouse = base_featurestouse + swc_featurestouse + ijh_featurestouse + cdr_featurestouse\n",
    "features = swc_features + ijh_features + cdr_features + odds_features + test_features\n",
    "features.append('PredSpread')\n",
    "\n",
    "################################################################################\n",
    "# Standardize data\n",
    "################################################################################\n",
    "\n",
    "####### Scale data select features\n",
    "standardscaler = StandardScaler()\n",
    "X_trainS = standardscaler.fit_transform(X_train[features])\n",
    "X_valS = standardscaler.transform(X_val[features])\n",
    "#X_testscaled = standardscaler.transform(X_test[featurestouse])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression #penalty, C, \n",
    "from sklearn.linear_model import ElasticNet #alpha, l1_ratio, warm_start, \n",
    "from sklearn.svm import SVC #C, kernel, degree-polynomial, gamma, \n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import SGDRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_final_clean['HomeWin'] = data_final_clean.apply(lambda x: 1 if x['target_margin'] > 0 else 0, axis=1)\n",
    "\n",
    "X_trainC = data_final_clean.loc[(data_final_clean['Season']<2016) & \\\n",
    "                     (data_final_clean['Week'] > 4)].\\\n",
    "                                       drop(['target_margin', 'HomeWin'], axis=1)\n",
    "    \n",
    "y_trainC = data_final_clean.loc[(data_final_clean['Season']<2016) & \\\n",
    "                     (data_final_clean['Week'] > 4), 'HomeWin']\n",
    "\n",
    "X_valC = data_final_clean.loc[(data_final_clean['Season']==2016) & \\\n",
    "                     (data_final_clean['Week'] > 4)].\\\n",
    "                                       drop(['target_margin', 'HomeWin'], axis=1)\n",
    "    \n",
    "y_valC = data_final_clean.loc[(data_final_clean['Season']==2016) & \\\n",
    "                     (data_final_clean['Week'] > 4), 'HomeWin']\n",
    "\n",
    "# Current season average margins diff\n",
    "X_trainC['CurSeason_HH_VA_mean_margin'] = (X_trainC['SpreadHomeInSeasonAvg'] \\\n",
    "                                    - X_trainC['SpreadVisInSeasonAvg']*-1)\n",
    "X_valC['CurSeason_HH_VA_mean_margin'] = (X_valC['SpreadHomeInSeasonAvg'] \\\n",
    "                                        - X_valC['SpreadVisInSeasonAvg']*-1)\n",
    "\n",
    "\n",
    "standardscaler = StandardScaler()\n",
    "X_trainscaledC = standardscaler.fit_transform(X_trainC[odds_featurestouse])\n",
    "X_valscaledC = standardscaler.transform(X_valC[odds_featurestouse])\n",
    "\n",
    "\n",
    "dataDict = {'X_train':X_trainscaledC, 'X_val':X_valscaledC, \n",
    "            'y_trainC':y_trainC, 'y_valC':y_valC, 'y_train':y_train, 'y_val':y_val}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doGridSearchC(X_train, y_train, X_val, y_val, estimator, param_grid, scorer):\n",
    "    X_train_val = np.vstack((X_train, X_val))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    val_fold = [-1]*len(X_train) + [0]*len(X_val) #0 corresponds to validation\n",
    "    grid = GridSearchCV(estimator,\n",
    "                        param_grid,\n",
    "                        return_train_score=False,\n",
    "                        cv = PredefinedSplit(test_fold=val_fold),\n",
    "                        refit = True,\n",
    "                        scoring = scorer)\n",
    "    grid.fit(X_train_val, y_train_val)\n",
    "    return grid.best_estimator_\n",
    "\n",
    "def chainClassiferGridSearch(nameC, nameR, estimatorC, estimatorR, paramC, dataDict=dataDict):\n",
    "\n",
    "    scorer = make_scorer(roc_auc_score, greater_is_better = True)\n",
    "    \n",
    "    X_trainC2 = dataDict['X_train'].copy()\n",
    "    X_valC2 = dataDict['X_val'].copy()\n",
    "    \n",
    "    classifer = doGridSearchC(X_trainC2, dataDict['y_trainC'], X_valC2, dataDict['y_valC'], estimatorC, paramC, scorer)\n",
    "    classifer = classifer.fit(X_trainC2, dataDict['y_trainC'])\n",
    "    predsTrain = classifer.predict_proba(X_trainC2)[:,1]\n",
    "    predsVal = classifer.predict_proba(X_valC2)[:,1]\n",
    "    print(nameC, roc_auc_score(dataDict['y_valC'], predsVal))\n",
    "\n",
    "    X_trainC2 = np.concatenate((X_trainC2, predsTrain.reshape(-1,1)), 1)\n",
    "    X_valC2 = np.concatenate((X_valC2, predsVal.reshape(-1,1)), 1)\n",
    "\n",
    "    regressor = estimatorR.fit(X_trainC2, dataDict['y_train'])\n",
    "    predsReg = regressor.predict(X_valC2)\n",
    "    print(nameR, mean_squared_error(dataDict['y_val'], predsReg))\n",
    "    return(classifer)\n",
    "\n",
    "def chainRegressorGridSearch(nameC, nameR, estimatorC, estimatorR, paramR, dataDict=dataDict):\n",
    "\n",
    "    scorer = make_scorer(mean_squared_error, greater_is_better = False)\n",
    "    \n",
    "    X_trainC2 = dataDict['X_train'].copy()\n",
    "    X_valC2 = dataDict['X_val'].copy()\n",
    "    \n",
    "    classifer = estimatorC.fit(X_trainC2, dataDict['y_trainC'])\n",
    "    predsTrain = classifer.predict_proba(X_trainC2)[:,1]\n",
    "    predsVal = classifer.predict_proba(X_valC2)[:,1]\n",
    "    print(nameC, roc_auc_score(dataDict['y_valC'], predsVal))\n",
    "\n",
    "    X_trainC2 = np.concatenate((X_trainC2, predsTrain.reshape(-1,1)), 1)\n",
    "    X_valC2 = np.concatenate((X_valC2, predsVal.reshape(-1,1)), 1)\n",
    "    \n",
    "    \n",
    "    regressor = doGridSearchC(X_trainC2, dataDict['y_train'], X_valC2, dataDict['y_val'], estimatorR, paramR, scorer)\n",
    "    regressor = regressor.fit(X_trainC2, dataDict['y_train'])\n",
    "    predsReg = regressor.predict(X_valC2)\n",
    "    print(nameR, mean_squared_error(dataDict['y_val'], predsReg))\n",
    "    return(regressor)\n",
    "\n",
    "\n",
    "def chainDoubleGridSearch(nameC, nameR, estimatorC, estimatorR, paramC, paramR, dataDict=dataDict):\n",
    "\n",
    "    scorerC = make_scorer(roc_auc_score, greater_is_better = True)\n",
    "    scorerR = make_scorer(mean_squared_error, greater_is_better = False)    \n",
    "    \n",
    "    X_trainC2 = dataDict['X_train'].copy()\n",
    "    X_valC2 = dataDict['X_val'].copy()\n",
    "    \n",
    "    classifer = doGridSearchC(X_trainC2, dataDict['y_trainC'], X_valC2, dataDict['y_valC'], estimatorC, paramC, scorerC)\n",
    "    classifer = classifer.fit(X_trainC2, dataDict['y_trainC'])\n",
    "    predsTrain = classifer.predict_proba(X_trainC2)[:,1]\n",
    "    predsVal = classifer.predict_proba(X_valC2)[:,1]\n",
    "    print(nameC, roc_auc_score(dataDict['y_valC'], predsVal))\n",
    "\n",
    "    X_trainC2 = np.concatenate((X_trainC2, predsTrain.reshape(-1,1)), 1)\n",
    "    X_trainC2 = np.concatenate((X_trainC2, np.where(predsTrain >0.5, 1, 0).reshape(-1,1)), 1)\n",
    "    X_valC2 = np.concatenate((X_valC2, predsVal.reshape(-1,1)), 1)\n",
    "    X_valC2 = np.concatenate((X_valC2, np.where(predsVal >0.5, 1, 0).reshape(-1,1)), 1)\n",
    "\n",
    "    regressor = doGridSearchC(X_trainC2, dataDict['y_train'], X_valC2, dataDict['y_val'], estimatorR, paramR, scorerR)\n",
    "    regrsregressorsor = regressor.fit(X_trainC2, dataDict['y_train'])\n",
    "    predsReg = regressor.predict(X_valC2)\n",
    "    print(nameR, mean_squared_error(dataDict['y_val'], predsReg))\n",
    "    return(classifer, regressor)\n",
    "\n",
    "def chainGridSearchWrapper(nameC, nameR, clssfier, estimatorC, estimatorR, params, dataDict=dataDict):\n",
    "    if len(params) == 2:\n",
    "        return chainDoubleGridSearch(nameC, nameR, estimatorC, estimatorR, params[0], params[1], dataDict)\n",
    "    elif clssfier:\n",
    "        return chainClassiferGridSearch(nameC, nameR, estimatorC, estimatorR, params, dataDict)\n",
    "    else:\n",
    "        return chainRegressorGridSearch(nameC, nameR, estimatorC, estimatorR, params, dataDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RFC 0.6493172860360361\n",
      "SGDR 277.4708015019177\n"
     ]
    }
   ],
   "source": [
    "rfc_sgd = chainGridSearchWrapper('RFC', 'SGDR', True,\n",
    "                          RandomForestClassifier(), \n",
    "                          SGDRegressor(loss='epsilon_insensitive', penalty='l1', max_iter=10000,\n",
    "                                       alpha=0.001, eta0=0.0001, learning_rate='invscaling'),\n",
    "                          [{'min_samples_leaf':np.arange(2, 20, 1),\n",
    "                            'min_samples_split':np.arange(2, 20, 1)}], \n",
    "                          dataDict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
