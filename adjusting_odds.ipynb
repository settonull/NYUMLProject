{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import itertools\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.getcwd()\n",
    "conf_dir = 'conferences'\n",
    "data_dir = os.path.join(root_dir, \"data\")\n",
    "\n",
    "\n",
    "def load_csvs(file_names):\n",
    "    \"\"\"Loads and concatentates csv's from a directory\"\"\"\n",
    "    df = pd.DataFrame()\n",
    "    for each_file in file_names:\n",
    "        new_df = pd.read_csv(each_file)\n",
    "        df = pd.concat([df, new_df])\n",
    "    return df\n",
    "\n",
    "def join_data(scores_df, stats_df, odds_df):\n",
    "    \"\"\"\n",
    "    Creates a unique key for each game using the date the game was played\n",
    "    and the home and away abbreviated names (Not all data sets have a HomeID\n",
    "    and AwayID)\n",
    "    \"\"\"\n",
    "\n",
    "    # Add dates to join on\n",
    "    scores_df['Year'] = scores_df['WeekDate'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").year)\n",
    "    scores_df['Month'] = scores_df['WeekDate'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").month)\n",
    "    scores_df['Day'] = scores_df['WeekDate'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").day)\n",
    "    stats_df['Year'] = stats_df['Start'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").year)\n",
    "    stats_df['Month'] = stats_df['Start'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").month)\n",
    "    stats_df['Day'] = stats_df['Start'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d %H:%M:%S\").day)\n",
    "    odds_df['Year'] = odds_df['DATE(date)'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").year)\n",
    "    odds_df['Month'] = odds_df['DATE(date)'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").month)\n",
    "    odds_df['Day'] = odds_df['DATE(date)'].apply(lambda x: datetime.strptime(x, \"%Y-%m-%d\").day)\n",
    "\n",
    "    # Join Data\n",
    "    data = scores_df.merge(\n",
    "        stats_df.drop(['Season', 'Start', 'Week'], axis=1),\n",
    "        left_on = ['Year', 'Month', 'Day', 'Home', 'Visiter'],\n",
    "        right_on = ['Year', 'Month', 'Day', 'Home', 'Away'])\n",
    "    data = data.merge(odds_df.drop(['DATE(date)', 'HomeScore', 'AwayScore'],\n",
    "                                    axis=1),\n",
    "        left_on = ['Year', 'Month', 'Day', 'Home', 'Visiter'],\n",
    "        right_on = ['Year', 'Month', 'Day', 'Home', 'Away'])\n",
    "\n",
    "    # Target feature\n",
    "    data['target_margin'] = data['HomeFinal'] - data['VisFinal']\n",
    "\n",
    "    # Other features\n",
    "    data['D1_Match'] = [True if not pd.isnull(x) else False for \\\n",
    "                        x in data['Spread_Mirage']]\n",
    "\n",
    "    return data\n",
    "\n",
    "# Load data locations\n",
    "scores_dir = 'scores_pe'\n",
    "stats_dir = 'stats'\n",
    "odds_dir = ''\n",
    "\n",
    "scores_names = glob.glob(os.path.join(root_dir, data_dir, scores_dir, \"scores_pythElo201?.csv\"))\n",
    "stats_names =  glob.glob(os.path.join(root_dir, data_dir, stats_dir, \"ncaastats201?.csv\"))\n",
    "odds_names = [os.path.join(root_dir, data_dir, odds_dir, \"NCAAF_Odds.csv\")]\n",
    "\n",
    "# Import data and join\n",
    "scores_df = load_csvs(scores_names)\n",
    "stats_df = load_csvs(stats_names)\n",
    "odds_df = load_csvs(odds_names)\n",
    "data = join_data(scores_df, stats_df, odds_df)\n",
    "\n",
    "spreads = data.set_index(['HomeID','VisID','Season','Week']).filter(regex=\"Spread_\")\n",
    "#m = spreads.mean(axis=1)\n",
    "#for i, col in enumerate(spreads):\n",
    "    # using i allows for duplicate columns\n",
    "    # inplace *may* not always work here, so IMO the next line is preferred\n",
    "    # df.iloc[:, i].fillna(m, inplace=True)\n",
    "    #spreads.iloc[:, i] = spreads.iloc[:, i].fillna(m)\n",
    "# spreads['target_margin'] = data['target_margin']\n",
    "#spreads.dropna(axis=0, inplace=True)\n",
    "spreads = spreads.join(pd.DataFrame(data.set_index(['HomeID','VisID','Season','Week'])['target_margin']))\n",
    "\n",
    "# Join Conference Data\n",
    "file = os.path.join(data_dir, conf_dir, \"mergedConferences.csv\")\n",
    "conf_df = pd.read_csv(file).drop_duplicates()\n",
    "spreads= spreads.reset_index().merge(conf_df,\n",
    "                                            left_on=['HomeID', 'Season'],\n",
    "                                            right_on=['ID','Year'],\n",
    "                                            suffixes=('','Home'))\n",
    "spreads = spreads.reset_index().merge(conf_df,\n",
    "                                            left_on=['VisID', 'Season'],\n",
    "                                            right_on=['ID','Year'],\n",
    "                                            suffixes=('','Vis'))\n",
    "spreads['Week'] = spreads['Week'].astype(int)\n",
    "spreads['Week'] = np.where(spreads['Season']==2016, spreads['Week'] - 1, spreads['Week'])\n",
    "spreads = spreads.set_index(['HomeID', 'VisID', 'Season', 'Week'])\n",
    "spreads = spreads.drop(['ID','Year','IDVis','index','Team','TeamVis','ConfVis','Year','YearVis'],1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SPLIT FOR USE IN R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for y, g in spreads2.groupby('Season'):\n",
    "    weeks = [group for _, group in g.groupby('Week')]\n",
    "    for i, w in enumerate(weeks):\n",
    "        i += 1\n",
    "        if i == 4:\n",
    "            pd.concat(weeks[:i]).to_csv('data/new_odds/pre/pre_{}/odds{}_{}.csv'.format(y, y, i), index=False)\n",
    "        elif i>4:\n",
    "            w.to_csv('data/new_odds/pre/pre_{}/odds{}_{}.csv'.format(y, y, i), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RECOMBINE AFTER R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_oddsList = [0 for i in range(4)]\n",
    "\n",
    "for i, yr in enumerate(range(2013,2017)):\n",
    "    lst_wk = spreads2.loc[spreads2['Season']==yr,'Week'].max()\n",
    "    new_oddsList[i] = pd.read_csv('data/new_odds/post/post_{}/odds{}_{}.csv'.format(yr, yr, lst_wk))\n",
    "    \n",
    "new_spreads = pd.concat(new_oddsList)\n",
    "new_spreads['Spread_Med2'] = new_spreads[casinos].median(axis=1)\n",
    "new_spreads['Spread_Mode2'] = new_spreads[casinos].mode(axis=1)[0]\n",
    "new_spreads.to_csv('data/new_odds/new_odds.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.kernel_ridge import KernelRidge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_casinos = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "               'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "               'Spread_SBG', 'Spread_BetUS']\n",
    "orig_med = ['Spread_Med']\n",
    "orig_mode = ['Spread_Mode']\n",
    "orig_summaries = ['Spread_Med', 'Spread_Mode']\n",
    "orig_total = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "               'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "               'Spread_SBG', 'Spread_BetUS', 'Spread_Med', 'Spread_Mode']\n",
    "\n",
    "new_med = ['Spread_Med2']\n",
    "new_mode = ['Spread_Mode2']\n",
    "new_summaries = ['Spread_Med2', 'Spread_Mode2']\n",
    "new_total = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "               'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "               'Spread_SBG', 'Spread_BetUS', 'Spread_Med', 'Spread_Mode']\n",
    "new_total_total = ['Spread_Mirage', 'Spread_Pinnacle', 'Spread_Sportsbet', \n",
    "               'Spread_Westgate', 'Spread_Station', 'Spread_SIA',\n",
    "               'Spread_SBG', 'Spread_BetUS', 'Spread_Med', 'Spread_Mode',\n",
    "                'Spread_Med2', 'Spread_Mode2']\n",
    "\n",
    "features = [('orig_casinos', orig_casinos), \n",
    "            ('orig_med', orig_med), ('orig_mode', orig_mode), ('orig_summaries', orig_summaries), ('orig_total', orig_total),\n",
    "            ('new_med', new_med), ('new_mode', new_mode), ('new_summaries', new_summaries), ('new_total', new_total)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_casinos: 261.4307500676896\n",
      "orig_med: 272.0562382987377\n",
      "orig_mode: 272.2678668409124\n",
      "orig_summaries: 272.2888260225782\n",
      "orig_total: 260.2619124221732\n",
      "new_med: 273.6773607758476\n",
      "new_mode: 274.2999240907326\n",
      "new_summaries: 273.496634545673\n",
      "new_total: 260.2619124221732\n"
     ]
    }
   ],
   "source": [
    "for name, feats in features:\n",
    "    new_feats = feats.copy()\n",
    "    new_feats.append('Season')\n",
    "    new_feats.append('target_margin')\n",
    "    new_spreadTemp = new_spreads[new_feats].copy().dropna()\n",
    "    \n",
    "    X_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,feats]\n",
    "    X_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,feats]\n",
    "    y_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,'target_margin']\n",
    "    y_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,'target_margin']\n",
    "    \n",
    "    standardscaler = StandardScaler()\n",
    "    X_trainS = standardscaler.fit_transform(X_train)\n",
    "    X_valS = standardscaler.transform(X_val)\n",
    "    \n",
    "    print('{}: {}' .format(name, mean_squared_error(y_val, BayesianRidge().fit(X_trainS, y_train).predict(X_valS))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_casinos\n",
      "{'alpha_1': 1.0, 'alpha_2': 5.6234132519034905e-08, 'lambda_1': 5.6234132519034905e-08, 'lambda_2': 1.0}\n",
      "MSE: 261.46309534670485\n",
      "*****************************************************\n",
      "orig_total\n",
      "{'alpha_1': 1.0, 'alpha_2': 5.6234132519034905e-08, 'lambda_1': 5.6234132519034905e-08, 'lambda_2': 1.0}\n",
      "MSE: 260.3163109699554\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "for name, feats in [('orig_casinos',orig_casinos), ('orig_total',orig_total)]:\n",
    "    new_feats = feats.copy()\n",
    "    new_feats.append('Season')\n",
    "    new_feats.append('target_margin')\n",
    "    new_spreadTemp = new_spreads[new_feats].copy().dropna()\n",
    "    \n",
    "    X_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,feats]\n",
    "    X_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,feats]\n",
    "    y_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,'target_margin']\n",
    "    y_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,'target_margin']\n",
    "    \n",
    "    standardscaler = StandardScaler()\n",
    "    X_trainS = standardscaler.fit_transform(X_train)\n",
    "    X_valS = standardscaler.transform(X_val)\n",
    "    \n",
    "    X_train_val = np.vstack((X_trainS, X_valS))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    val_fold = [-1]*len(X_trainS) + [0]*len(X_valS) #0 corresponds to validation\n",
    "    grid = GridSearchCV(BayesianRidge(),\n",
    "                        [{'alpha_1':10**np.arange(0, 3, 0.25),\n",
    "                          'alpha_2':10**np.arange(-10, -7, 0.25),\n",
    "                          'lambda_1':10**np.arange(-10, -7, 0.25),\n",
    "                          'lambda_2':10**np.arange(0, 3, 0.25)}],\n",
    "                        return_train_score=False,\n",
    "                        cv = PredefinedSplit(test_fold=val_fold),\n",
    "                        refit = True,\n",
    "                        scoring = make_scorer(mean_squared_error, greater_is_better = False))\n",
    "    grid.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    bestimator = grid.best_estimator_\n",
    "    print(name)\n",
    "    print(grid.best_params_ )\n",
    "    print('MSE: {}' .format(mean_squared_error(y_val, bestimator.fit(X_trainS, y_train).predict(X_valS))))\n",
    "    print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "orig_casinos\n",
      "{'alpha': 1e-09, 'eta0': 0.0001, 'learning_rate': 'invscaling'}\n",
      "MSE: 256.8414429758759\n",
      "*****************************************************\n",
      "orig_total\n",
      "{'alpha': 0.001, 'eta0': 0.0001, 'learning_rate': 'invscaling'}\n",
      "MSE: 255.35641933873944\n",
      "*****************************************************\n"
     ]
    }
   ],
   "source": [
    "for name, feats in [('orig_casinos',orig_casinos), ('orig_total',orig_total)]:\n",
    "    new_feats = feats.copy()\n",
    "    new_feats.append('Season')\n",
    "    new_feats.append('target_margin')\n",
    "    new_spreadTemp = new_spreads[new_feats].copy().dropna()\n",
    "    \n",
    "    X_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,feats]\n",
    "    X_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,feats]\n",
    "    y_train = new_spreadTemp.loc[new_spreadTemp['Season']<2016,'target_margin']\n",
    "    y_val = new_spreadTemp.loc[new_spreadTemp['Season']==2016,'target_margin']\n",
    "    \n",
    "    standardscaler = StandardScaler()\n",
    "    X_trainS = standardscaler.fit_transform(X_train)\n",
    "    X_valS = standardscaler.transform(X_val)\n",
    "    \n",
    "    X_train_val = np.vstack((X_trainS, X_valS))\n",
    "    y_train_val = np.concatenate((y_train, y_val))\n",
    "    val_fold = [-1]*len(X_trainS) + [0]*len(X_valS) #0 corresponds to validation\n",
    "    grid = GridSearchCV(SGDRegressor(loss='epsilon_insensitive', penalty='l1', max_iter=10000),\n",
    "                        [{'alpha':10**np.arange(-10,1,1.0),\n",
    "                          'eta0':10**np.arange(-5,2,1.0),\n",
    "                          'learning_rate':['constant','optimal','invscaling']}],\n",
    "                        return_train_score=False,\n",
    "                        cv = PredefinedSplit(test_fold=val_fold),\n",
    "                        refit = True,\n",
    "                        scoring = make_scorer(mean_squared_error, greater_is_better = False))\n",
    "    grid.fit(X_train_val, y_train_val)\n",
    "    \n",
    "    bestimator = grid.best_estimator_\n",
    "    print(name)\n",
    "    print(grid.best_params_ )\n",
    "    print('MSE: {}' .format(mean_squared_error(y_val, bestimator.fit(X_trainS, y_train).predict(X_valS))))\n",
    "    print('*****************************************************')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.26430062120519215"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bestimator.score(X_valS, y_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
